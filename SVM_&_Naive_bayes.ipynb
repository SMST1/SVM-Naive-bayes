{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*1.What is a Support Vector Machine (SVM)?*\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used primarily for classification tasks, though it can also be used for regression. Its main goal is to find the optimal hyperplane that best separates data points of different classes in a high-dimensional space.\n",
        "\n",
        "Key Concepts:\n",
        "Hyperplane: A decision boundary that separates classes. In 2D, it's a line; in 3D, a plane; and in higher dimensions, it's called a hyperplane.\n",
        "\n",
        "Support Vectors: The data points that are closest to the hyperplane. These points are critical because they define the position and orientation of the hyperplane.\n",
        "\n",
        "Margin: The distance between the hyperplane and the nearest data points from each class. SVM aims to maximize this margin for better generalization.\n",
        "\n",
        "Linear vs Non-linear SVM:\n",
        "\n",
        "Linear SVM: Works well when data is linearly separable.\n",
        "\n",
        "Non-linear SVM: Uses kernel functions (like RBF, polynomial, sigmoid) to transform data into a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "Kernel Trick: A mathematical technique that allows SVMs to operate in a high-dimensional space without explicitly computing the coordinates. It enables SVMs to handle non-linearly separable data efficiently.\n",
        "\n",
        "*2. What is the difference between Hard Margin and Soft Margin SVM ?*\n",
        "\n",
        "The difference between Hard Margin and Soft Margin SVM lies in how strictly the algorithm separates the classes when training the model.\n",
        "\n",
        "ğŸ”· Hard Margin SVM\n",
        "Assumes that the data is perfectly linearly separable.\n",
        "\n",
        "The algorithm finds the maximum-margin hyperplane with no tolerance for misclassification.\n",
        "\n",
        "No data points are allowed inside the margin or on the wrong side of the hyperplane.\n",
        "\n",
        "Sensitive to noise and outliersâ€”even a single misclassified point can make a hard margin infeasible.\n",
        "\n",
        "Use case: Clean, linearly separable datasets without outliers.\n",
        "\n",
        "ğŸ”¶ Soft Margin SVM\n",
        "Allows some misclassifications to achieve better generalization on real-world, noisy data.\n",
        "\n",
        "Introduces a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing classification errors:\n",
        "\n",
        "High C â†’ less tolerance for misclassification (closer to hard margin).\n",
        "\n",
        "Low C â†’ more tolerance for errors, allowing a wider margin.\n",
        "\n",
        "More robust in practice and applicable to most real-world problems.\n",
        "\n",
        "Use case: Datasets with noise, overlap, or imperfect separability.\n",
        "\n",
        "*3. What is the mathematical intuition behind SVM ?*\n",
        "\n",
        "The mathematical intuition behind SVM revolves around finding the optimal hyperplane that best separates data points from different classes by maximizing the marginâ€”the distance between the hyperplane and the nearest points (support vectors) from each class.\n",
        "SVMs maximize margin and use support vectors to define the decision boundary. The optimization balances margin width (simplicity) and classification error (accuracy), with extensions to non-linear problems via kernel functions\n",
        "\n",
        "*4. What is the role of Lagrange Multipliers in SVM ?*\n",
        "\n",
        "The role of Lagrange multipliers in SVM is to solve the constrained optimization problem efficiently using duality theory from convex optimization. This approach converts the original primal problem into a dual problem that's often easier to solveâ€”especially when using kernels for non-linear SVMs.\n",
        "\n",
        "ğŸ”¹ Recap: The Primal Problem (Hard Margin SVM)\n",
        "min\n",
        "â¡\n",
        "ğ‘¤\n",
        ",\n",
        "ğ‘\n",
        "\n",
        "1\n",
        "2\n",
        "âˆ¥\n",
        "ğ‘¤\n",
        "âˆ¥\n",
        "2\n",
        "subjectÂ to\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "(\n",
        "ğ‘¤\n",
        "ğ‘‡\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "+\n",
        "ğ‘\n",
        ")\n",
        "â‰¥\n",
        "1\n",
        "âˆ€\n",
        "ğ‘–\n",
        "w,b\n",
        "min\n",
        "â€‹\n",
        "\n",
        "2\n",
        "1\n",
        "â€‹\n",
        " âˆ¥wâˆ¥\n",
        "2\n",
        "\n",
        "subjectÂ toÂ y\n",
        "i\n",
        "â€‹\n",
        " (w\n",
        "T\n",
        " x\n",
        "i\n",
        "â€‹\n",
        " +b)â‰¥1âˆ€i\n",
        "ğŸ”¹ Step 1: Introduce Lagrange Multipliers\n",
        "We incorporate the constraints using Lagrange multipliers\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "â‰¥\n",
        "0\n",
        "Î±\n",
        "i\n",
        "â€‹\n",
        " â‰¥0:\n",
        "\n",
        "ğ¿\n",
        "(\n",
        "ğ‘¤\n",
        ",\n",
        "ğ‘\n",
        ",\n",
        "ğ›¼\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "âˆ¥\n",
        "ğ‘¤\n",
        "âˆ¥\n",
        "2\n",
        "âˆ’\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "[\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "(\n",
        "ğ‘¤\n",
        "ğ‘‡\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "+\n",
        "ğ‘\n",
        ")\n",
        "âˆ’\n",
        "1\n",
        "]\n",
        "L(w,b,Î±)=\n",
        "2\n",
        "1\n",
        "â€‹\n",
        " âˆ¥wâˆ¥\n",
        "2\n",
        " âˆ’\n",
        "i=1\n",
        "âˆ‘\n",
        "n\n",
        "â€‹\n",
        " Î±\n",
        "i\n",
        "â€‹\n",
        " [y\n",
        "i\n",
        "â€‹\n",
        " (w\n",
        "T\n",
        " x\n",
        "i\n",
        "â€‹\n",
        " +b)âˆ’1]\n",
        "Here:\n",
        "\n",
        "ğ¿\n",
        "L is the Lagrangian function\n",
        "\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "Î±\n",
        "i\n",
        "â€‹\n",
        "  are Lagrange multipliers, one per constraint\n",
        "\n",
        "ğŸ”¹ Step 2: Solve the Dual Problem\n",
        "We take the partial derivatives of\n",
        "ğ¿\n",
        "L with respect to\n",
        "ğ‘¤\n",
        "w and\n",
        "ğ‘\n",
        "b, set them to zero, and substitute back into\n",
        "ğ¿\n",
        "L, giving the dual optimization problem:\n",
        "\n",
        "max\n",
        "â¡\n",
        "ğ›¼\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "âˆ’\n",
        "1\n",
        "2\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "âˆ‘\n",
        "ğ‘—\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "ğ›¼\n",
        "ğ‘—\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "ğ‘¦\n",
        "ğ‘—\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "ğ‘‡\n",
        "ğ‘¥\n",
        "ğ‘—\n",
        "subjectÂ to:\n",
        "âˆ‘\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "=\n",
        "0\n",
        ",\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "â‰¥\n",
        "0\n",
        "Î±\n",
        "max\n",
        "â€‹\n",
        "  \n",
        "i=1\n",
        "âˆ‘\n",
        "n\n",
        "â€‹\n",
        " Î±\n",
        "i\n",
        "â€‹\n",
        " âˆ’\n",
        "2\n",
        "1\n",
        "â€‹\n",
        "  \n",
        "i=1\n",
        "âˆ‘\n",
        "n\n",
        "â€‹\n",
        "  \n",
        "j=1\n",
        "âˆ‘\n",
        "n\n",
        "â€‹\n",
        " Î±\n",
        "i\n",
        "â€‹\n",
        " Î±\n",
        "j\n",
        "â€‹\n",
        " y\n",
        "i\n",
        "â€‹\n",
        " y\n",
        "j\n",
        "â€‹\n",
        " x\n",
        "i\n",
        "T\n",
        "â€‹\n",
        " x\n",
        "j\n",
        "â€‹\n",
        "\n",
        "subjectÂ to:\n",
        "i=1\n",
        "âˆ‘\n",
        "n\n",
        "â€‹\n",
        " Î±\n",
        "i\n",
        "â€‹\n",
        " y\n",
        "i\n",
        "â€‹\n",
        " =0,Î±\n",
        "i\n",
        "â€‹\n",
        " â‰¥0\n",
        "This dual form has key advantages:\n",
        "\n",
        "Depends only on dot products of input vectors, enabling the kernel trick.\n",
        "\n",
        "Often more computationally efficient for high-dimensional data.\n",
        "\n",
        "\n",
        "*5. What are Support Vectors in SVM ?*\n",
        "\n",
        "Support Vectors in SVM are the critical data points that lie closest to the decision boundary (the hyperplane). They are the \"supporting\" elements that define the position and orientation of the optimal hyperplane.\n",
        "\n",
        "ğŸ”¹ Key Characteristics:\n",
        "Closest Points to the Hyperplane:\n",
        "\n",
        "These are the data points that lie exactly on the margin boundaries in the hard-margin case, or within or on the margin in the soft-margin case.\n",
        "\n",
        "Non-zero Lagrange Multipliers:\n",
        "\n",
        "In the dual formulation of SVM, only data points with non-zero\n",
        "ğ›¼\n",
        "ğ‘–\n",
        "Î±\n",
        "i\n",
        "â€‹\n",
        "  are support vectors.\n",
        "\n",
        "These are the only points that affect the modelâ€”others are irrelevant to the final decision boundary.\n",
        "\n",
        "Decision Boundary Depends Only on Them:\n",
        "\n",
        "The optimal hyperplane is completely determined by the support vectors. If you remove non-support vectors, the hyperplane remains unchanged.\n",
        "\n",
        "*6. What is a Support Vector Classifier (SVC) ?*\n",
        "\n",
        "A Support Vector Classifier (SVC) is the practical implementation of the Support Vector Machine (SVM) algorithm for classification tasks.\n",
        "\n",
        "While \"SVM\" refers to the broader concept or algorithm (including regression and classification), SVC specifically refers to its use in classification, especially as implemented in tools like scikit-learn's sklearn.svm.SVC.\n",
        "\n",
        "*7. What is a Support Vector Regressor (SVR) ?*\n",
        "\n",
        "A Support Vector Regressor (SVR) is the regression counterpart of Support Vector Machines (SVM). Instead of classifying data into categories, SVR is used to predict continuous values.\n",
        "\n",
        "ğŸ”¹ Core Idea of SVR\n",
        "SVR tries to find a function\n",
        "ğ‘“\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "f(x) that deviates from the actual target\n",
        "ğ‘¦\n",
        "y by at most\n",
        "ğœ–\n",
        "Ïµ for all training data, while being as flat (simple) as possible.\n",
        "\n",
        "Think of it as fitting a tube of width\n",
        "2\n",
        "ğœ–\n",
        "2Ïµ around the data. The goal is to include as many data points as possible inside this tube (i.e., within the tolerance\n",
        "ğœ–\n",
        "Ïµ), and penalize those that fall outside.\n",
        "\n",
        "*8. What is the Kernel Trick in SVM ?*\n",
        "\n",
        "The Kernel Trick is a powerful mathematical technique in SVM that allows it to perform non-linear classification or regression without explicitly transforming the data into higher-dimensional space.\n",
        "\n",
        "ğŸ”¹ The Problem\n",
        "Some datasets are not linearly separable in their original input space. To separate them with a linear hyperplane, youâ€™d need to map them to a higher-dimensional space where they become linearly separable.\n",
        "\n",
        "ğŸ”¹ The Challenge\n",
        "Explicitly mapping data to a higher-dimensional space (via a function\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "Ï•(x)) is:\n",
        "\n",
        "Computationally expensive\n",
        "\n",
        "Sometimes even infeasible, especially if the feature space is infinite-dimensional\n",
        "\n",
        "ğŸ”¹ The Solution: The Kernel Trick\n",
        "Instead of computing\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "ğ‘‡\n",
        "ğœ™\n",
        "(\n",
        "ğ‘§\n",
        ")\n",
        "Ï•(x)\n",
        "T\n",
        " Ï•(z), we use a kernel function:\n",
        "\n",
        "ğ¾\n",
        "(\n",
        "ğ‘¥\n",
        ",\n",
        "ğ‘§\n",
        ")\n",
        "=\n",
        "âŸ¨\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        ",\n",
        "ğœ™\n",
        "(\n",
        "ğ‘§\n",
        ")\n",
        "âŸ©\n",
        "K(x,z)=âŸ¨Ï•(x),Ï•(z)âŸ©\n",
        "This lets SVM work implicitly in high-dimensional space without ever computing\n",
        "ğœ™\n",
        "(\n",
        "ğ‘¥\n",
        ")\n",
        "Ï•(x) directly.\n",
        "\n",
        "*9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel ?*\n",
        "\n",
        " 1. Linear Kernel\n",
        "Definition:\n",
        "ğ¾\n",
        "(\n",
        "ğ‘¥\n",
        ",\n",
        "ğ‘§\n",
        ")\n",
        "=\n",
        "ğ‘¥\n",
        "ğ‘‡\n",
        "ğ‘§\n",
        "K(x,z)=x\n",
        "T\n",
        " z\n",
        "Intuition:\n",
        "No transformation; the decision boundary is a straight line or hyperplane.\n",
        "\n",
        "Assumes data is linearly separable.\n",
        "\n",
        "Pros:\n",
        "Fast and simple\n",
        "\n",
        "Works well for high-dimensional, sparse data (e.g., text classification)\n",
        "\n",
        "No hyperparameters to tune\n",
        "\n",
        "Cons:\n",
        "Cannot capture non-linear relationships\n",
        "\n",
        "Use Case:\n",
        "Linearly separable data\n",
        "\n",
        "Text data (e.g., TF-IDF vectors)\n",
        "\n",
        "\n",
        " 2. Polynomial Kernel\n",
        "Definition:\n",
        "ğ¾\n",
        "(\n",
        "ğ‘¥\n",
        ",\n",
        "ğ‘§\n",
        ")\n",
        "=\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘‡\n",
        "ğ‘§\n",
        "+\n",
        "ğ‘\n",
        ")\n",
        "ğ‘‘\n",
        "K(x,z)=(x\n",
        "T\n",
        " z+c)\n",
        "d\n",
        "\n",
        "Intuition:\n",
        "Maps input data into a higher-degree polynomial space\n",
        "\n",
        "Captures non-linear patterns using curved decision boundaries\n",
        "\n",
        "Pros:\n",
        "More flexible than linear\n",
        "\n",
        "Can model interactions between features\n",
        "\n",
        "Cons:\n",
        "Sensitive to degree (d) and coefficient (c)\n",
        "\n",
        "Higher-degree polynomials can lead to overfitting\n",
        "\n",
        "Slower than linear kernel\n",
        "\n",
        "Use Case:\n",
        "Data with non-linear but structured relationships\n",
        "\n",
        "When interactions between features are important\n",
        "\n",
        "\n",
        " 3. RBF (Radial Basis Function) Kernel\n",
        "Definition:\n",
        "ğ¾\n",
        "(\n",
        "ğ‘¥\n",
        ",\n",
        "ğ‘§\n",
        ")\n",
        "=\n",
        "exp\n",
        "â¡\n",
        "(\n",
        "âˆ’\n",
        "ğ›¾\n",
        "âˆ¥\n",
        "ğ‘¥\n",
        "âˆ’\n",
        "ğ‘§\n",
        "âˆ¥\n",
        "2\n",
        ")\n",
        "K(x,z)=exp(âˆ’Î³âˆ¥xâˆ’zâˆ¥\n",
        "2\n",
        " )\n",
        "Intuition:\n",
        "Measures similarity based on distance\n",
        "\n",
        "Projects data into an infinite-dimensional space\n",
        "\n",
        "Can model very complex boundaries\n",
        "\n",
        "Pros:\n",
        "Highly flexible and powerful\n",
        "\n",
        "Can fit very complex, non-linear data\n",
        "\n",
        "Cons:\n",
        "Requires tuning of gamma and C\n",
        "\n",
        "Can overfit if not properly regularized\n",
        "\n",
        "Less interpretable\n",
        "\n",
        "Use Case:\n",
        "General-purpose, non-linear problems\n",
        "\n",
        "When you donâ€™t know the data structure well in advance\n",
        "\n",
        "*10. What is the effect of the C parameter in SVM ?*\n",
        "\n",
        "The C parameter in SVM controls the trade-off between maximizing the margin and minimizing classification error. It is a regularization parameter that balances model complexity against training accuracy.\n",
        "\n",
        "effect\n",
        "\n",
        "| C Value     | Behavior                             | Model Effect                                                                    |\n",
        "| ----------- | ------------------------------------ | ------------------------------------------------------------------------------- |\n",
        "| **Large C** | Less tolerance for misclassification | Narrow margin, fits training data tightly â†’ can **overfit**                     |\n",
        "| **Small C** | More tolerance for misclassification | Wider margin, allows some errors â†’ can **underfit**, but **generalizes better** |\n",
        "\n",
        "*11. What is the role of the Gamma parameter in RBF Kernel SVM ?*\n",
        "\n",
        "The Gamma (Î³) parameter in the RBF kernel (Radial Basis Function kernel) plays a crucial role in controlling the spread or influence of a single training point in the decision boundary.\n",
        "\n",
        "What Does Gamma Control?\n",
        "Gamma defines how much influence a single training point has on the decision boundary.\n",
        "\n",
        "It controls the width of the Gaussian (RBF) function used to measure the similarity between data points.\n",
        "\n",
        "Specifically, it controls the distance within which points are considered influential in shaping the decision boundary.\n",
        "\n",
        "Mathematically, for two points\n",
        "ğ‘¥\n",
        "x and\n",
        "ğ‘§\n",
        "z, the RBF kernel function is:\n",
        "\n",
        "ğ¾\n",
        "(\n",
        "ğ‘¥\n",
        ",\n",
        "ğ‘§\n",
        ")\n",
        "=\n",
        "exp\n",
        "â¡\n",
        "(\n",
        "âˆ’\n",
        "ğ›¾\n",
        "âˆ¥\n",
        "ğ‘¥\n",
        "âˆ’\n",
        "ğ‘§\n",
        "âˆ¥\n",
        "2\n",
        ")\n",
        "K(x,z)=exp(âˆ’Î³âˆ¥xâˆ’zâˆ¥\n",
        "2\n",
        " )\n",
        "As\n",
        "ğ›¾\n",
        "Î³ increases, the influence of individual points becomes more localized, and the model is more sensitive to small variations in the data.\n",
        "\n",
        "As\n",
        "ğ›¾\n",
        "Î³ decreases, the influence of each point spreads out, leading to a smoother decision boundary that may be less sensitive to noise.\n",
        "\n",
        "Intuitive Explanation\n",
        "High Gamma:\n",
        "\n",
        "Each training point's influence is very localized, so the decision boundary is influenced by individual points. This can lead to a complicated and overfitted model.\n",
        "\n",
        "In this case, the model can fit the training data tightly, even capturing noise and outliers, making it less likely to generalize well on new data.\n",
        "\n",
        "Low Gamma:\n",
        "\n",
        "Each point's influence is spread over a larger area, resulting in a smoother decision boundary. This leads to a more generalized model that might underfit the data if the boundary is too simple for the underlying patterns.\n",
        "\n",
        "*12. What is the NaÃ¯ve Bayes classifier, and why is it called \"NaÃ¯ve\" ?*\n",
        "\n",
        "The NaÃ¯ve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem, which is used for classification tasks. It is particularly popular for text classification (e.g., spam filtering, sentiment analysis) due to its simplicity and efficiency.\n",
        "\n",
        "Bayes' Theorem:\n",
        "At its core, the NaÃ¯ve Bayes classifier applies Bayes' Theorem to calculate the probability of each class given the features of the input data. Bayes' Theorem is:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "âˆ£\n",
        "ğ‘‹\n",
        ")\n",
        "=\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "â‹…\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "P(C\n",
        "k\n",
        "â€‹\n",
        " âˆ£X)=\n",
        "P(X)\n",
        "P(Xâˆ£C\n",
        "k\n",
        "â€‹\n",
        " )â‹…P(C\n",
        "k\n",
        "â€‹\n",
        " )\n",
        "â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "âˆ£\n",
        "ğ‘‹\n",
        ")\n",
        "P(C\n",
        "k\n",
        "â€‹\n",
        " âˆ£X): The posterior probability of class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        "  given the features\n",
        "ğ‘‹\n",
        "X.\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "P(Xâˆ£C\n",
        "k\n",
        "â€‹\n",
        " ): The likelihood of the features\n",
        "ğ‘‹\n",
        "X given class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " .\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "P(C\n",
        "k\n",
        "â€‹\n",
        " ): The prior probability of class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " .\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "P(X): The marginal likelihood or probability of the features (which acts as a normalization factor).\n",
        "\n",
        "The classifier predicts the class that maximizes the posterior probability\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "âˆ£\n",
        "ğ‘‹\n",
        ")\n",
        "P(C\n",
        "k\n",
        "â€‹\n",
        " âˆ£X).\n",
        "\n",
        "\n",
        "\n",
        "The \"NaÃ¯ve\" Assumption:\n",
        "The \"naÃ¯ve\" part comes from the assumption that the features are conditionally independent given the class label. This assumption means that the presence (or absence) of a particular feature in the input data is assumed to be independent of the presence or absence of other features, given the class.\n",
        "\n",
        "This is a strong and often unrealistic assumption, hence the term \"naÃ¯ve.\" Despite its simplicity, this assumption works surprisingly well in many practical situations, especially when the features are weakly correlated.\n",
        "\n",
        "Mathematically, for\n",
        "ğ‘›\n",
        "n features\n",
        "ğ‘‹\n",
        "=\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        ",\n",
        "ğ‘¥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        ")\n",
        "X=(x\n",
        "1\n",
        "â€‹\n",
        " ,x\n",
        "2\n",
        "â€‹\n",
        " ,...,x\n",
        "n\n",
        "â€‹\n",
        " ), the classifier assumes:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "=\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        ",\n",
        "ğ‘¥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "=\n",
        "âˆ\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "P(Xâˆ£C\n",
        "k\n",
        "â€‹\n",
        " )=P(x\n",
        "1\n",
        "â€‹\n",
        " ,x\n",
        "2\n",
        "â€‹\n",
        " ,...,x\n",
        "n\n",
        "â€‹\n",
        " âˆ£C\n",
        "k\n",
        "â€‹\n",
        " )=\n",
        "i=1\n",
        "âˆ\n",
        "n\n",
        "â€‹\n",
        " P(x\n",
        "i\n",
        "â€‹\n",
        " âˆ£C\n",
        "k\n",
        "â€‹\n",
        " )\n",
        "This drastically simplifies the computation of the likelihood\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘‹\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "P(Xâˆ£C\n",
        "k\n",
        "â€‹\n",
        " ), as you only need to consider the individual likelihoods of each feature\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  given the class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " .\n",
        "\n",
        " The classifier is called \"naÃ¯ve\" because of the strong assumption that all features are independent given the class label, which is typically not true in real-world data. For example, in text classification, the presence of one word (e.g., \"free\") often depends on the presence of another word (e.g., \"money\"), but the NaÃ¯ve Bayes classifier ignores these dependencies.\n",
        "\n",
        "Despite this unrealistic assumption, the model works quite well in practice, especially when the features are relatively independent or when dependencies are weak.\n",
        "\n",
        "*13. What is Bayesâ€™ Theorem ?*\n",
        "\n",
        "Bayes' Theorem is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis (or event) based on new evidence. It provides a way to compute the posterior probability of an event, given prior knowledge and new data.\n",
        "\n",
        "Bayes' Theorem Formula\n",
        "Mathematically, Bayes' Theorem is expressed as:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ´\n",
        "âˆ£\n",
        "ğµ\n",
        ")\n",
        "=\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğµ\n",
        "âˆ£\n",
        "ğ´\n",
        ")\n",
        "â‹…\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ´\n",
        ")\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğµ\n",
        ")\n",
        "P(Aâˆ£B)=\n",
        "P(B)\n",
        "P(Bâˆ£A)â‹…P(A)\n",
        "â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ´\n",
        "âˆ£\n",
        "ğµ\n",
        ")\n",
        "P(Aâˆ£B) is the posterior probability: the probability of event\n",
        "ğ´\n",
        "A happening given that event\n",
        "ğµ\n",
        "B has occurred.\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğµ\n",
        "âˆ£\n",
        "ğ´\n",
        ")\n",
        "P(Bâˆ£A) is the likelihood: the probability of observing event\n",
        "ğµ\n",
        "B given that event\n",
        "ğ´\n",
        "A is true.\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ´\n",
        ")\n",
        "P(A) is the prior probability: the initial probability of event\n",
        "ğ´\n",
        "A before observing event\n",
        "ğµ\n",
        "B.\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğµ\n",
        ")\n",
        "P(B) is the marginal likelihood (or evidence): the total probability of observing event\n",
        "ğµ\n",
        "B (across all possible causes).\n",
        "\n",
        "*14. Explain the differences between Gaussian NaÃ¯ve Bayes, Multinomial NaÃ¯ve Bayes, and Bernoulli NaÃ¯ve Bayes ?*\n",
        "\n",
        "The NaÃ¯ve Bayes classifier is a probabilistic model based on Bayes' Theorem. Different types of NaÃ¯ve Bayes classifiers are used depending on the type of data being analyzed. The main variants of NaÃ¯ve Bayes are Gaussian NaÃ¯ve Bayes, Multinomial NaÃ¯ve Bayes, and Bernoulli NaÃ¯ve Bayes. These variants differ in the assumptions they make about the underlying data distributions.\n",
        "\n",
        "\n",
        "1. Gaussian NaÃ¯ve Bayes\n",
        "Data Assumption:\n",
        "Assumes that the features (i.e., the input data) are continuous and follow a Gaussian (Normal) distribution for each class.\n",
        "\n",
        "Mathematical Model:\n",
        "For a given feature\n",
        "ğ‘¥\n",
        "x, the likelihood of\n",
        "ğ‘¥\n",
        "x given the class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        "  is modeled as a Gaussian distribution:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "=\n",
        "1\n",
        "2\n",
        "ğœ‹\n",
        "ğœ\n",
        "2\n",
        "exp\n",
        "â¡\n",
        "(\n",
        "âˆ’\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğœ‡\n",
        ")\n",
        "2\n",
        "2\n",
        "ğœ\n",
        "2\n",
        ")\n",
        "P(x\n",
        "i\n",
        "â€‹\n",
        " âˆ£C\n",
        "k\n",
        "â€‹\n",
        " )=\n",
        "2Ï€Ïƒ\n",
        "2\n",
        "\n",
        "â€‹\n",
        "\n",
        "1\n",
        "â€‹\n",
        " exp(âˆ’\n",
        "2Ïƒ\n",
        "2\n",
        "\n",
        "(x\n",
        "i\n",
        "â€‹\n",
        " âˆ’Î¼)\n",
        "2\n",
        "\n",
        "â€‹\n",
        " )\n",
        "Where:\n",
        "\n",
        "ğœ‡\n",
        "Î¼ is the mean of the feature\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  for class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " ,\n",
        "\n",
        "ğœ\n",
        "2\n",
        "Ïƒ\n",
        "2\n",
        "  is the variance of\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  for class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " .\n",
        "\n",
        "Use Case:\n",
        "Best suited for continuous, real-valued data, where the distribution of the features is approximately Gaussian.\n",
        "\n",
        "Example:\n",
        "Predicting the price of a house based on continuous features like square footage, number of rooms, and age of the house.\n",
        "\n",
        "ğŸ”¹ 2. Multinomial NaÃ¯ve Bayes\n",
        "Data Assumption:\n",
        "Assumes that the features are discrete and follow a Multinomial distribution. This is common when the features represent counts or frequencies of occurrences.\n",
        "\n",
        "Mathematical Model:\n",
        "Given a set of features\n",
        "ğ‘¥\n",
        "=\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        ",\n",
        "ğ‘¥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        ")\n",
        "x=(x\n",
        "1\n",
        "â€‹\n",
        " ,x\n",
        "2\n",
        "â€‹\n",
        " ,...,x\n",
        "n\n",
        "â€‹\n",
        " ), the likelihood of the feature vector given a class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        "  is modeled using the Multinomial distribution:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "=\n",
        "âˆ\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "(\n",
        "ğ‘“\n",
        "ğ‘–\n",
        ")\n",
        "!\n",
        "(\n",
        "ğ‘“\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        ")\n",
        "!\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "!\n",
        "â‹…\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "P(xâˆ£C\n",
        "k\n",
        "â€‹\n",
        " )=\n",
        "i=1\n",
        "âˆ\n",
        "n\n",
        "â€‹\n",
        "  \n",
        "(f\n",
        "i\n",
        "â€‹\n",
        " âˆ’x\n",
        "i\n",
        "â€‹\n",
        " )!x\n",
        "i\n",
        "â€‹\n",
        " !\n",
        "(f\n",
        "i\n",
        "â€‹\n",
        " )!\n",
        "â€‹\n",
        " â‹…p\n",
        "i\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘“\n",
        "ğ‘–\n",
        "f\n",
        "i\n",
        "â€‹\n",
        "  is the total count of feature\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        " ,\n",
        "\n",
        "ğ‘\n",
        "ğ‘–\n",
        "p\n",
        "i\n",
        "â€‹\n",
        "  is the probability of feature\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  occurring in class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " ,\n",
        "\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  is the frequency of feature\n",
        "ğ‘–\n",
        "i in a given sample.\n",
        "\n",
        "Use Case:\n",
        "Most commonly used for document classification problems, where the features are word counts or term frequencies (TF) from text data.\n",
        "\n",
        "Example:\n",
        "Classifying a document as spam or not based on the frequency of words (e.g., â€œmoney,â€ â€œoffer,â€ etc.).\n",
        "\n",
        "ğŸ”¹ 3. Bernoulli NaÃ¯ve Bayes\n",
        "Data Assumption:\n",
        "Assumes that the features are binary (i.e., they can take values 0 or 1), representing the presence or absence of a feature in a given class.\n",
        "\n",
        "Mathematical Model:\n",
        "Given a binary feature vector\n",
        "ğ‘¥\n",
        "=\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        ",\n",
        "ğ‘¥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        ")\n",
        "x=(x\n",
        "1\n",
        "â€‹\n",
        " ,x\n",
        "2\n",
        "â€‹\n",
        " ,...,x\n",
        "n\n",
        "â€‹\n",
        " ), the likelihood of the feature vector given a class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        "  is modeled using the Bernoulli distribution:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "âˆ£\n",
        "ğ¶\n",
        "ğ‘˜\n",
        ")\n",
        "=\n",
        "âˆ\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘\n",
        "ğ‘–\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "(\n",
        "1\n",
        "âˆ’\n",
        "ğ‘\n",
        "ğ‘–\n",
        ")\n",
        "1\n",
        "âˆ’\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "P(xâˆ£C\n",
        "k\n",
        "â€‹\n",
        " )=\n",
        "i=1\n",
        "âˆ\n",
        "n\n",
        "â€‹\n",
        " p\n",
        "i\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "\n",
        "â€‹\n",
        " (1âˆ’p\n",
        "i\n",
        "â€‹\n",
        " )\n",
        "1âˆ’x\n",
        "i\n",
        "â€‹\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘\n",
        "ğ‘–\n",
        "p\n",
        "i\n",
        "â€‹\n",
        "  is the probability of feature\n",
        "ğ‘–\n",
        "i being present in class\n",
        "ğ¶\n",
        "ğ‘˜\n",
        "C\n",
        "k\n",
        "â€‹\n",
        " ,\n",
        "\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "x\n",
        "i\n",
        "â€‹\n",
        "  is a binary indicator for feature\n",
        "ğ‘–\n",
        "i (1 if present, 0 if absent).\n",
        "\n",
        "Use Case:\n",
        "Suitable for binary or Boolean features, such as text classification problems where each feature represents the presence or absence of a particular word (often in bag-of-words models).\n",
        "\n",
        "Example:\n",
        "Classifying emails as spam or not based on the presence or absence of specific words (e.g., â€œfree,â€ â€œdiscountâ€).\n",
        "\n",
        "*15. When should you use Gaussian NaÃ¯ve Bayes over other variants ?*\n",
        "\n",
        "You should consider using Gaussian NaÃ¯ve Bayes (GNB) over other variants of NaÃ¯ve Bayes (like Multinomial NaÃ¯ve Bayes or Bernoulli NaÃ¯ve Bayes) when your data exhibits the following characteristics:\n",
        "\n",
        "1. Continuous Data:\n",
        "Gaussian NaÃ¯ve Bayes is ideal when your features are continuous variables (i.e., real-valued numbers).\n",
        "\n",
        "This variant assumes that the data follows a Gaussian (Normal) distribution, so it works well when your features naturally follow this type of distribution, such as measurements like height, weight, or temperature.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Predicting house prices based on continuous features like square footage, number of rooms, and age of the house.\n",
        "\n",
        "Classifying medical conditions based on continuous health data like blood pressure, cholesterol level, and body mass index (BMI).\n",
        "\n",
        "2. Feature Distribution Resembling Normal Distribution:\n",
        "If you have continuous features and you suspect that each feature in a class follows a Gaussian distribution (or approximately so), then Gaussian NaÃ¯ve Bayes is the right choice.\n",
        "\n",
        "This is often the case in many real-world scenarios, where many natural phenomena follow a normal distribution due to the Central Limit Theorem.\n",
        "\n",
        "Example:\n",
        "\n",
        "Predicting a student's exam score based on continuous study hours and prior performance, where the distribution of these features is likely normal.\n",
        "\n",
        "3. Large Datasets:\n",
        "GNB tends to perform well on large datasets with many continuous features because it assumes simple relationships (mean and variance) between the features and the class, which is computationally efficient.\n",
        "\n",
        "The model is easy to train and doesn't require complex optimization, making it a good option for large-scale problems.\n",
        "\n",
        "Example:\n",
        "\n",
        "Classifying customer behaviors in an e-commerce store based on continuous features such as browsing time, number of items viewed, and transaction amounts.\n",
        "\n",
        "4. Computational Efficiency:\n",
        "Gaussian NaÃ¯ve Bayes is computationally efficient and performs well in terms of speed, especially when the dataset contains many features. It calculates probabilities using simple parameters: mean and variance.\n",
        "\n",
        "This makes it faster than more complex algorithms like SVM or Random Forest, which is useful when you're dealing with large datasets or require real-time predictions.\n",
        "\n",
        "Example:\n",
        "\n",
        "Classifying sensor data from IoT devices, where data from sensors (such as temperature or humidity) are continuous and need quick classification.\n",
        "\n",
        "5. When Data is Well-Conditioned and Low Noise:\n",
        "Gaussian NaÃ¯ve Bayes assumes that features are independent and normally distributed within each class. If your data is well-conditioned (features are relatively clean, without much noise or outliers), Gaussian NaÃ¯ve Bayes will perform well.\n",
        "\n",
        "If the dataset has a lot of outliers or the features do not follow a Gaussian distribution, you may need to consider data transformation (e.g., normalizing or transforming features to make them more Gaussian) or choose a different variant like Multinomial NaÃ¯ve Bayes.\n",
        "\n",
        "Example:\n",
        "\n",
        "Predicting quality control in a manufacturing process where each feature (e.g., size, weight, and material properties) follows a normal distribution with few outliers.\n",
        "\n",
        "*16. What are the key assumptions made by NaÃ¯ve Bayes ?*\n",
        "\n",
        "The NaÃ¯ve Bayes classifier makes several key assumptions that are crucial to how it models data and computes probabilities. These assumptions simplify the model significantly, making it fast and effective in many scenariosâ€”but they also introduce limitations.\n",
        "\n",
        "Here are the key assumptions made by NaÃ¯ve Bayes:\n",
        "\n",
        "ğŸ”¹ 1. Feature Independence (The \"NaÃ¯ve\" Assumption)\n",
        "Assumption: All features are conditionally independent of each other given the class label.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        ",\n",
        "ğ‘¥\n",
        "2\n",
        ",\n",
        ".\n",
        ".\n",
        ".\n",
        ",\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "=\n",
        "âˆ\n",
        "ğ‘–\n",
        "=\n",
        "1\n",
        "ğ‘›\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "P(x\n",
        "1\n",
        "â€‹\n",
        " ,x\n",
        "2\n",
        "â€‹\n",
        " ,...,x\n",
        "n\n",
        "â€‹\n",
        " âˆ£y)=\n",
        "i=1\n",
        "âˆ\n",
        "n\n",
        "â€‹\n",
        " P(x\n",
        "i\n",
        "â€‹\n",
        " âˆ£y)\n",
        "This means the model assumes that knowing the value of one feature gives no information about any other feature, once we know the class.\n",
        "\n",
        "Impact: This is a strong and often unrealistic assumption in real-world data (where features are often correlated), but it works surprisingly well in practiceâ€”especially when the correlations among features are similar across classes.\n",
        "\n",
        "ğŸ”¹ 2. Class Conditional Independence of Features\n",
        "This is an extension of the first assumption: for each class label\n",
        "ğ‘¦\n",
        "y, the joint probability of the feature vector\n",
        "ğ‘¥\n",
        "âƒ—\n",
        "x\n",
        "  is equal to the product of the individual feature probabilities conditioned on that class.\n",
        "\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "âƒ—\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "=\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "1\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "â‹…\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "2\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "â‹…\n",
        "â‹¯\n",
        "â‹…\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘›\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "P(\n",
        "x\n",
        " âˆ£y)=P(x\n",
        "1\n",
        "â€‹\n",
        " âˆ£y)â‹…P(x\n",
        "2\n",
        "â€‹\n",
        " âˆ£y)â‹…â‹¯â‹…P(x\n",
        "n\n",
        "â€‹\n",
        " âˆ£y)\n",
        "Why it matters: It allows the model to compute the likelihood of a data point very efficiently, even in high dimensions.\n",
        "\n",
        "ğŸ”¹ 3. Correct Model of Feature Distribution\n",
        "Depending on the variant of NaÃ¯ve Bayes, it makes an assumption about the distribution of features for each class:\n",
        "\n",
        "Gaussian NaÃ¯ve Bayes: Assumes each continuous feature follows a normal (Gaussian) distribution.\n",
        "\n",
        "Multinomial NaÃ¯ve Bayes: Assumes features represent count data, typically used in text classification.\n",
        "\n",
        "Bernoulli NaÃ¯ve Bayes: Assumes binary features (e.g., presence or absence of a word).\n",
        "\n",
        "If the actual distribution deviates too much from the assumed model, the classifierâ€™s performance may degrade.\n",
        "\n",
        "ğŸ”¹ 4. All Features Contribute Equally and Independently\n",
        "Each feature contributes independently and equally to the final classification. That means no feature is treated as inherently more important unless reflected in its conditional probability.\n",
        "\n",
        "This can be limiting when some features are highly predictive and others are mostly noise.\n",
        "\n",
        "ğŸ”¹ 5. No Interaction Terms or Feature Combinations\n",
        "Since features are treated as independent, NaÃ¯ve Bayes does not model interactions (e.g.,\n",
        "ğ‘¥\n",
        "1\n",
        "Ã—\n",
        "ğ‘¥\n",
        "2\n",
        "x\n",
        "1\n",
        "â€‹\n",
        " Ã—x\n",
        "2\n",
        "â€‹\n",
        "  effects).\n",
        "\n",
        "This can be a limitation in domains where the interaction between features is critical for accurate prediction.\n",
        "\n",
        "*17. What are the advantages and disadvantages of NaÃ¯ve Bayes ?*\n",
        "\n",
        "Advantages of NaÃ¯ve Bayes\n",
        "1. Simple and Fast\n",
        "Training and prediction are extremely fast, especially with large datasets.\n",
        "\n",
        "Time complexity is linear in the number of features and training examples.\n",
        "\n",
        "2. Works Well with High-Dimensional Data\n",
        "Performs well even when the number of features is very large (e.g., in text classification).\n",
        "\n",
        "Often used in spam detection, sentiment analysis, and document categorization.\n",
        "\n",
        "3. Effective with Small Data\n",
        "Requires relatively little training data to estimate model parameters (means and variances or frequencies).\n",
        "\n",
        "Performs well with limited training data, especially when the assumptions hold.\n",
        "\n",
        "4. Robust to Irrelevant Features\n",
        "Even if some features are irrelevant (donâ€™t contribute meaningfully), it can still perform well.\n",
        "\n",
        "5. Handles Both Binary and Multiclass Classification\n",
        "Naturally supports multiple classes without any modification.\n",
        "\n",
        "6. Probabilistic Output\n",
        "Provides not just a classification, but also the probability of each class, which is useful for decision-making under uncertainty.\n",
        "\n",
        "âŒ Disadvantages of NaÃ¯ve Bayes\n",
        "1. Strong Independence Assumption\n",
        "Assumes conditional independence between features, which is often violated in practice.\n",
        "\n",
        "If features are highly correlated, performance may degrade.\n",
        "\n",
        "2. Zero-Frequency Problem\n",
        "If a feature value is not present in the training data for a given class, the model assigns it zero probability, which can lead to incorrect classifications.\n",
        "\n",
        "This is usually handled with Laplace smoothing (adding 1 to all counts).\n",
        "\n",
        "3. Limited Expressiveness\n",
        "Doesnâ€™t model interactions between features (e.g., \"if feature A and B occur together\").\n",
        "\n",
        "Can underperform when decision boundaries are highly nonlinear or complex.\n",
        "\n",
        "4. Not Good with Continuous, Non-Gaussian Data (Unless Transformed)\n",
        "Gaussian NaÃ¯ve Bayes assumes that features are normally distributed. If the actual data distribution is non-Gaussian, accuracy may drop.\n",
        "\n",
        "May require feature scaling or transformations.\n",
        "\n",
        "5. Overconfident Probabilities\n",
        "Due to the independence assumption, the probability estimates can be unrealistically high or low, making them poorly calibrated.\n",
        "\n",
        "*18. Why is NaÃ¯ve Bayes a good choice for text classification ?*\n",
        "\n",
        "NaÃ¯ve Bayes is a popular and effective choice for text classification due to several key strengths that align well with the nature of text data:\n",
        "\n",
        "âœ… Why NaÃ¯ve Bayes Works Well for Text Classification\n",
        "1. High Dimensionality Compatibility\n",
        "Text data often has thousands to millions of features (words or tokens).\n",
        "\n",
        "NaÃ¯ve Bayes handles this effortlessly because its computational complexity is linear with respect to the number of features.\n",
        "\n",
        "2. Assumption of Feature Independence is Reasonable\n",
        "In text classification, features (words) are typically modeled as independent given the document class.\n",
        "\n",
        "While words in natural language aren't truly independent, this naÃ¯ve assumption surprisingly works well in practiceâ€”especially with bag-of-words or TF-IDF representations.\n",
        "\n",
        "3. Robust to Irrelevant Features\n",
        "In text, many words (features) may be irrelevant to the class.\n",
        "\n",
        "NaÃ¯ve Bayes is robust to noisy and irrelevant features because it evaluates each word independently, so a few noisy words donâ€™t drastically skew predictions.\n",
        "\n",
        "4. Efficient with Sparse Data\n",
        "Text datasets are typically sparse (most word counts are zero).\n",
        "\n",
        "NaÃ¯ve Bayes naturally handles sparse matrices well, making it memory- and speed-efficient.\n",
        "\n",
        "5. Works Well with Small Data\n",
        "Even with small training sets, NaÃ¯ve Bayes can build effective models, especially when classes are well-separated by distinctive words.\n",
        "\n",
        "6. Supports Multiclass Classification\n",
        "Many real-world text problems (e.g., news topic classification, intent recognition) involve multiple classes.\n",
        "\n",
        "NaÃ¯ve Bayes naturally supports multiclass classification without any special modification.\n",
        "\n",
        "7. Simple, Interpretable, and Fast\n",
        "NaÃ¯ve Bayes is easy to implement, fast to train and predict, and the learned model is interpretable (you can inspect which words are most informative for each class).\n",
        "\n",
        "*19.  Compare SVM and NaÃ¯ve Bayes for classification tasks ?*\n",
        "\n",
        "Core Comparison: SVM vs. NaÃ¯ve Bayes\n",
        "\n",
        "\n",
        "| **Aspect**                   | **SVM (Support Vector Machine)**                             | **NaÃ¯ve Bayes**                                                    |\n",
        "| ---------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------ |\n",
        "| **Model Type**               | Discriminative (learns decision boundary)                    | Generative (learns data distribution given a class)                |\n",
        "| **Assumptions**              | No specific assumptions; finds optimal separating hyperplane | Assumes conditional independence between features                  |\n",
        "| **Performance**              | High accuracy, especially with clear margin separation       | Fast and performs well with simple and high-dimensional data       |\n",
        "| **Speed (Training)**         | Slower, especially on large datasets                         | Extremely fast (especially with Multinomial or Bernoulli variants) |\n",
        "| **Speed (Prediction)**       | Slower, especially with non-linear kernels                   | Very fast                                                          |\n",
        "| **Handling High Dimensions** | Good (especially with kernel trick)                          | Excellent (text data, sparse matrices)                             |\n",
        "| **Noise Sensitivity**        | Sensitive to noisy data and outliers                         | More robust to noise due to probabilistic nature                   |\n",
        "| **Interpretability**         | Less interpretable (complex decision boundaries)             | More interpretable (based on feature likelihoods)                  |\n",
        "| **Probabilistic Output**     | Not inherently probabilistic (but can be calibrated)         | Outputs direct class probabilities                                 |\n",
        "| **Multi-class Support**      | Requires strategy (e.g., One-vs-All, One-vs-One)             | Naturally supports multi-class classification                      |\n",
        "\n",
        "\n",
        "*20. How does Laplace Smoothing help in NaÃ¯ve Bayes?*\n",
        "\n",
        "Laplace Smoothing (also known as add-one smoothing) helps in NaÃ¯ve Bayes by addressing the zero-frequency problem, which occurs when a categorical feature value is not observed in the training data for a given class. Without smoothing, this leads to zero probability for the entire observationâ€”effectively causing the model to ignore otherwise relevant information.\n",
        "\n",
        "\n",
        "Laplace Smoothing adds a small constant (usually 1) to each count to avoid zero probabilities.\n",
        "\n",
        "For a categorical feature:\n",
        "ğ‘ƒ\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "=\n",
        "ğ‘¤\n",
        "âˆ£\n",
        "ğ‘¦\n",
        ")\n",
        "=\n",
        "count\n",
        "(\n",
        "ğ‘¤\n",
        ",\n",
        "ğ‘¦\n",
        ")\n",
        "+\n",
        "1\n",
        "âˆ‘\n",
        "ğ‘¤\n",
        "â€²\n",
        "count\n",
        "(\n",
        "ğ‘¤\n",
        "â€²\n",
        ",\n",
        "ğ‘¦\n",
        ")\n",
        "+\n",
        "ğ‘‰\n",
        "P(x\n",
        "i\n",
        "â€‹\n",
        " =wâˆ£y)=\n",
        "âˆ‘\n",
        "w\n",
        "â€²\n",
        "\n",
        "â€‹\n",
        " count(w\n",
        "â€²\n",
        " ,y)+V\n",
        "count(w,y)+1\n",
        "â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "count\n",
        "(\n",
        "ğ‘¤\n",
        ",\n",
        "ğ‘¦\n",
        ")\n",
        "count(w,y) = number of times word\n",
        "ğ‘¤\n",
        "w appears in class\n",
        "ğ‘¦\n",
        "y\n",
        "\n",
        "ğ‘‰\n",
        "V = number of possible unique feature values (e.g., vocabulary size)\n",
        "\n",
        "Adding 1 ensures no probability is zero.\n",
        "\n",
        "Without smoothing: unseen words/features have zero probability, ruining predictions.\n",
        "\n",
        "With smoothing: all words/features have non-zero probabilities, so rare or unseen values are still assigned small but nonzero likelihood."
      ],
      "metadata": {
        "id": "6YnX4raLBpfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21  Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Step 2: Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train an SVM classifier\n",
        "svm_clf = SVC(kernel='linear')  # You can try 'rbf', 'poly', etc.\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on the test set\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"SVM Classifier Accuracy on Iris Dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "CXNpz63AW4jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Step 2: Split dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Step 4: Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Step 5: Compare and print accuracies\n",
        "print(f\"Accuracy with Linear Kernel: {acc_linear:.2f}\")\n",
        "print(f\"Accuracy with RBF Kernel:    {acc_rbf:.2f}\")\n",
        "\n",
        "if acc_linear > acc_rbf:\n",
        "    print(\"â¡ï¸ Linear kernel performed better.\")\n",
        "elif acc_rbf > acc_linear:\n",
        "    print(\"â¡ï¸ RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"â¡ï¸ Both kernels performed equally.\")\n"
      ],
      "metadata": {
        "id": "uXiiDnIxXBvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23 Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train-test split (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Feature scaling (important for SVR)\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).ravel()\n",
        "y_test_scaled = scaler_y.transform(y_test.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Step 4: Train the SVR model (with RBF kernel)\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train_scaled, y_train_scaled)\n",
        "\n",
        "# Step 5: Predict and inverse transform the target\n",
        "y_pred_scaled = svr.predict(X_test_scaled)\n",
        "y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
        "\n",
        "# Step 6: Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE) of SVR on Housing Dataset: {mse:.3f}\")\n"
      ],
      "metadata": {
        "id": "Dec--EOkXQ2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24 Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Generate a synthetic 2D dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
        "    n_clusters_per_class=1, class_sep=1.5, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM with a Polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Create meshgrid for plotting decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Step 5: Plot the decision boundary and data points\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', label=\"Train\")\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm, marker='x', label=\"Test\")\n",
        "plt.title(\"SVM with Polynomial Kernel (degree=3)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HFpjjHQmXZzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25  Write a Python program to train a Gaussian NaÃ¯ve Bayes classifier on the Breast Cancer dataset and evaluate accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Gaussian NaÃ¯ve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy of Gaussian NaÃ¯ve Bayes on Breast Cancer Dataset: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "VaVbQMIOXgtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26 Write a Python program to train a Multinomial NaÃ¯ve Bayes classifier for text classification using the 20 Newsgroups dataset.\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all')  # Use 'all' for training and testing\n",
        "\n",
        "X = newsgroups.data  # Text data\n",
        "y = newsgroups.target  # Labels (20 classes)\n",
        "\n",
        "# Step 2: Split dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Convert text data to feature vectors using CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')  # Remove common stop words\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 4: Train Multinomial NaÃ¯ve Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train_vec, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate the model\n",
        "y_pred = nb_classifier.predict(X_test_vec)\n",
        "\n",
        "# Step 6: Evaluate accuracy and print classification report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Multinomial NaÃ¯ve Bayes on 20 Newsgroups dataset: {accuracy:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n"
      ],
      "metadata": {
        "id": "KC4ZzHItXrEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27 Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Generate a synthetic 2D dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
        "                            n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
        "\n",
        "# Step 2: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define different values for the C parameter\n",
        "C_values = [0.1, 1, 10]\n",
        "\n",
        "# Step 4: Create subplots to visualize decision boundaries\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# Step 5: Train SVM Classifiers with different C values and plot decision boundaries\n",
        "for i, C in enumerate(C_values):\n",
        "    # Train SVM classifier\n",
        "    svm_clf = SVC(kernel='linear', C=C)\n",
        "    svm_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Create meshgrid for plotting decision boundary\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                         np.linspace(y_min, y_max, 300))\n",
        "    Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    ax = axes[i]\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k', label=\"Train\")\n",
        "    ax.set_title(f\"SVM with C={C}\")\n",
        "    ax.set_xlabel(\"Feature 1\")\n",
        "    ax.set_ylabel(\"Feature 2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JYDIrllBYAV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28 Write a Python program to train a Bernoulli NaÃ¯ve Bayes classifier for binary classification on a dataset with binary features\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Step 1: Generate a synthetic dataset with binary features\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_informative=3, n_redundant=0,\n",
        "                            n_clusters_per_class=1, n_classes=2, random_state=42)\n",
        "\n",
        "# Convert features to binary (Bernoulli Naive Bayes works with binary features)\n",
        "X = (X > 0).astype(int)\n",
        "\n",
        "# Step 2: Split dataset into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Bernoulli NaÃ¯ve Bayes classifier\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = bnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print the accuracy\n",
        "print(f\"Accuracy of Bernoulli NaÃ¯ve Bayes on Binary Feature Dataset: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Ttz1JWDZYYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29.  Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic 2D dataset\n",
        "X, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0,\n",
        "                            n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
        "\n",
        "# Step 2: Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM on unscaled data\n",
        "svm_unscaled = SVC(kernel='linear')\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# Step 4: Apply feature scaling using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Train SVM on scaled data\n",
        "svm_scaled = SVC(kernel='linear')\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 6: Print accuracies for comparison\n",
        "print(f\"Accuracy on unscaled data: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy on scaled data: {accuracy_scaled:.4f}\")\n",
        "\n",
        "# Step 7: Visualize the decision boundaries\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot for unscaled data\n",
        "axes[0].set_title(\"SVM with Unscaled Data\")\n",
        "axes[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "axes[0].scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=plt.cm.coolwarm, marker='x', label=\"Test Data\")\n",
        "axes[0].set_xlabel(\"Feature 1\")\n",
        "axes[0].set_ylabel(\"Feature 2\")\n",
        "axes[0].legend()\n",
        "\n",
        "# Plot for scaled data\n",
        "axes[1].set_title(\"SVM with Scaled Data\")\n",
        "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, cmap=plt.cm.coolwarm, marker='x', label=\"Test Data\")\n",
        "axes[1].set_xlabel(\"Feature 1 (Scaled)\")\n",
        "axes[1].set_ylabel(\"Feature 2 (Scaled)\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-abjdoxMYiNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 Write a Python program to train a Gaussian NaÃ¯ve Bayes model and compare the predictions before and after Laplace Smoothing.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Generate a synthetic 2D dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0,\n",
        "                            n_clusters_per_class=1, n_classes=2, random_state=42)\n",
        "\n",
        "# Step 2: Split dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train Gaussian NaÃ¯ve Bayes model without Laplace smoothing\n",
        "gnb_no_smoothing = GaussianNB()\n",
        "gnb_no_smoothing.fit(X_train, y_train)\n",
        "y_pred_no_smoothing = gnb_no_smoothing.predict(X_test)\n",
        "accuracy_no_smoothing = accuracy_score(y_test, y_pred_no_smoothing)\n",
        "\n",
        "# Step 4: Train Gaussian NaÃ¯ve Bayes model with Laplace smoothing (var_smoothing parameter)\n",
        "gnb_with_smoothing = GaussianNB(var_smoothing=1e-9)  # var_smoothing adds smoothing to variance\n",
        "gnb_with_smoothing.fit(X_train, y_train)\n",
        "y_pred_with_smoothing = gnb_with_smoothing.predict(X_test)\n",
        "accuracy_with_smoothing = accuracy_score(y_test, y_pred_with_smoothing)\n",
        "\n",
        "# Step 5: Print the accuracies for comparison\n",
        "print(f\"Accuracy without Laplace Smoothing: {accuracy_no_smoothing:.4f}\")\n",
        "print(f\"Accuracy with Laplace Smoothing: {accuracy_with_smoothing:.4f}\")\n"
      ],
      "metadata": {
        "id": "QPZA4ZONYu3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31 Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf', 'poly']\n",
        "}\n",
        "\n",
        "# Step 4: Initialize the SVM classifier\n",
        "svm = SVC()\n",
        "\n",
        "# Step 5: Perform GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Get the best parameters and evaluate the model\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best hyperparameters found: {best_params}\")\n",
        "\n",
        "# Step 7: Train the best model on the full training data\n",
        "best_svm = grid_search.best_estimator_\n",
        "\n",
        "# Step 8: Evaluate the model on the test set\n",
        "y_pred = best_svm.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 9: Print the accuracy of the best model\n",
        "print(f\"Accuracy of the tuned SVM model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "yAHRbLVmZE31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Generate an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5,\n",
        "                            n_classes=2, weights=[0.9, 0.1], flip_y=0, random_state=42)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train SVM Classifier without class weighting (baseline model)\n",
        "svm_no_weight = SVC(kernel='linear', class_weight=None)\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "\n",
        "# Step 4: Train SVM Classifier with class weighting (to handle the imbalance)\n",
        "svm_with_weight = SVC(kernel='linear', class_weight='balanced')\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "\n",
        "# Step 5: Print the results\n",
        "print(f\"Accuracy without class weighting: {accuracy_no_weight:.4f}\")\n",
        "print(f\"Accuracy with class weighting: {accuracy_with_weight:.4f}\")\n",
        "\n",
        "# Step 6: Print classification reports for more detailed evaluation\n",
        "print(\"\\nClassification Report (No Weighting):\")\n",
        "print(classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "print(\"\\nClassification Report (With Class Weighting):\")\n",
        "print(classification_report(y_test, y_pred_with_weight))\n"
      ],
      "metadata": {
        "id": "7orzr6omZSkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33.  Write a Python program to implement a NaÃ¯ve Bayes classifier for spam detection using email data\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the dataset (SMS Spam Collection Dataset)\n",
        "# Download the dataset from https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "# For simplicity, we will assume the dataset is loaded in a CSV format.\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "\n",
        "# Load the dataset\n",
        "# Dataset is in tab-separated values format\n",
        "df = pd.read_csv(\"SMSSpamCollection\", sep='\\t', header=None, names=['label', 'message'])\n",
        "\n",
        "# Step 2: Preprocessing\n",
        "# Convert the labels into binary form: 1 for 'spam' and 0 for 'ham'\n",
        "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Step 3: Split dataset into features (X) and labels (y)\n",
        "X = df['message']\n",
        "y = df['label']\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 5: Convert text data into numerical format using CountVectorizer (Bag of Words)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Train the NaÃ¯ve Bayes classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Step 7: Make predictions on the test set\n",
        "y_pred = nb.predict(X_test_vectorized)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Step 9: Print a classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))\n"
      ],
      "metadata": {
        "id": "NeQBANifZbh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34. Write a Python program to train an SVM Classifier and a NaÃ¯ve Bayes Classifier on the same dataset and compare their accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "\n",
        "# Step 4: Train the NaÃ¯ve Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "y_pred_nb = nb_classifier.predict(X_test)\n",
        "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "\n",
        "# Step 5: Compare the results\n",
        "print(f\"Accuracy of SVM Classifier: {svm_accuracy:.4f}\")\n",
        "print(f\"Accuracy of NaÃ¯ve Bayes Classifier: {nb_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "O-4jt9WQZ4xg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35. Write a Python program to perform feature selection before training a NaÃ¯ve Bayes classifier and compare results\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the NaÃ¯ve Bayes Classifier without feature selection\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "y_pred_no_selection = nb_classifier.predict(X_test)\n",
        "accuracy_no_selection = accuracy_score(y_test, y_pred_no_selection)\n",
        "\n",
        "# Step 4: Perform Feature Selection using SelectKBest\n",
        "# Select top 2 features using the f_classif statistical test\n",
        "selector = SelectKBest(score_func=f_classif, k=2)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Step 5: Train the NaÃ¯ve Bayes Classifier on the selected features\n",
        "nb_classifier_selected = GaussianNB()\n",
        "nb_classifier_selected.fit(X_train_selected, y_train)\n",
        "y_pred_with_selection = nb_classifier_selected.predict(X_test_selected)\n",
        "accuracy_with_selection = accuracy_score(y_test, y_pred_with_selection)\n",
        "\n",
        "# Step 6: Compare the results\n",
        "print(f\"Accuracy without feature selection: {accuracy_no_selection:.4f}\")\n",
        "print(f\"Accuracy with feature selection: {accuracy_with_selection:.4f}\")\n"
      ],
      "metadata": {
        "id": "OCer-USQaDFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier using One-vs-Rest (OvR)\n",
        "svm_ovr = SVC(decision_function_shape='ovr', kernel='linear')\n",
        "svm_ovr.fit(X_train, y_train)\n",
        "y_pred_ovr = svm_ovr.predict(X_test)\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "\n",
        "# Step 4: Train the SVM Classifier using One-vs-One (OvO)\n",
        "svm_ovo = SVC(decision_function_shape='ovo', kernel='linear')\n",
        "svm_ovo.fit(X_train, y_train)\n",
        "y_pred_ovo = svm_ovo.predict(X_test)\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "\n",
        "# Step 5: Compare the results\n",
        "print(f\"Accuracy of SVM with One-vs-Rest: {accuracy_ovr:.4f}\")\n",
        "print(f\"Accuracy of SVM with One-vs-One: {accuracy_ovo:.4f}\")\n"
      ],
      "metadata": {
        "id": "v1VKtFEbaNgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier with a Linear Kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Step 4: Train the SVM Classifier with a Polynomial Kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3)  # degree=3 is a common choice for polynomial kernel\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "\n",
        "# Step 5: Train the SVM Classifier with an RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Step 6: Compare the results\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with Polynomial Kernel: {accuracy_poly:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.4f}\")\n"
      ],
      "metadata": {
        "id": "XiFjAhyJafO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Initialize Stratified K-Fold Cross-Validation (5-fold)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 3: Initialize the SVM Classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Step 4: Perform Stratified K-Fold Cross-Validation\n",
        "accuracies = []\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    # Split data into training and testing sets for the current fold\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the SVM Classifier on the training data\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the test data\n",
        "    y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy for the current fold\n",
        "    fold_accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(fold_accuracy)\n",
        "\n",
        "# Step 5: Compute the average accuracy across all folds\n",
        "average_accuracy = np.mean(accuracies)\n",
        "\n",
        "# Step 6: Print the results\n",
        "print(f\"Accuracy for each fold: {accuracies}\")\n",
        "print(f\"Average accuracy across all folds: {average_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "GU9XI-k2bOkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39. Write a Python program to train a NaÃ¯ve Bayes classifier using different prior probabilities and compare performance\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Define different prior probabilities (uniform vs custom)\n",
        "# 3.1 Uniform prior (default)\n",
        "nb_uniform = GaussianNB()\n",
        "\n",
        "# 3.2 Custom priors (e.g., giving more weight to class 0)\n",
        "priors_custom = [0.4, 0.3, 0.3]  # Class 0 has a prior probability of 0.4, etc.\n",
        "nb_custom = GaussianNB(priors=priors_custom)\n",
        "\n",
        "# Step 4: Train and evaluate NaÃ¯ve Bayes with uniform prior probabilities\n",
        "nb_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = nb_uniform.predict(X_test)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "# Step 5: Train and ev\n"
      ],
      "metadata": {
        "id": "q7P7XW09bY7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train an SVM Classifier without RFE\n",
        "svm_no_rfe = SVC(kernel='linear')\n",
        "svm_no_rfe.fit(X_train, y_train)\n",
        "y_pred_no_rfe = svm_no_rfe.predict(X_test)\n",
        "accuracy_no_rfe = accuracy_score(y_test, y_pred_no_rfe)\n",
        "\n",
        "# Step 4: Perform Recursive Feature Elimination (RFE)\n",
        "# Initialize SVM Classifier\n",
        "svm_rfe = SVC(kernel='linear')\n",
        "\n",
        "# Initialize RFE with the SVM classifier and select top 10 features\n",
        "rfe = RFE(estimator=svm_rfe, n_features_to_select=10)\n",
        "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "# Train the SVM classifier with selected features from RFE\n",
        "svm_rfe.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "\n",
        "# Step 5: Compare the results\n",
        "print(f\"Accuracy without RFE: {accuracy_no_rfe:.4f}\")\n",
        "print(f\"Accuracy with RFE (top 10 features): {accuracy_rfe:.4f}\")\n"
      ],
      "metadata": {
        "id": "N9Kugt88bjwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41 Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model using Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "hu8N-_uUb1Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42. Write a Python program to train a NaÃ¯ve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the NaÃ¯ve Bayes Classifier\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make probability predictions on the test set\n",
        "y_pred_prob = nb_classifier.predict_proba(X_test)\n",
        "\n",
        "# Step 5: Compute Log Loss (Cross-Entropy Loss)\n",
        "loss = log_loss(y_test, y_pred_prob)\n",
        "\n",
        "# Step 6: Print the Log Loss\n",
        "print(f\"Log Loss (Cross-Entropy Loss): {loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "R2rWXfWNb-Xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43.  Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize the confusion matrix using seaborn's heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.title(\"Confusion Matrix for SVM Classifier\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A7OeZHe-cHrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Regressor (SVR) with an RBF kernel\n",
        "svr = SVR(kernel='rbf')\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate the model using Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the MAE\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
      ],
      "metadata": {
        "id": "42MjImAycSLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 45. Write a Python program to train a NaÃ¯ve Bayes classifier and evaluate its performance using the ROC-AUC score.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the NaÃ¯ve Bayes classifier (GaussianNB)\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make probability predictions on the test set\n",
        "y_pred_prob = nb_classifier.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Step 5: Compute the ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
        "\n",
        "# Step 6: Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "bGvqZXpJcdJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 46.  Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the SVM Classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear', probability=True)  # Set probability=True to get predicted probabilities\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get the predicted probabilities for the positive class\n",
        "y_pred_prob = svm_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Step 6: Compute the area under the Precision-Recall curve (PR AUC)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "# Step 7: Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
        "plt.fill_between(recall, precision, color='lightblue', alpha=0.5)\n",
        "plt.title('Precision-Recall Curve for SVM Classifier')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OTyEjQjMcnd-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}